{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "0.57.0dev0+1688.gd9ec7cb28\n",
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce RTX 3060 Ti'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 7\n",
      "                                    UUID: GPU-3b406151-7e9b-b2be-c6a2-c00c7bf371b1\n",
      "                                Watchdog: Enabled\n",
      "                            Compute Mode: WDDM\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from numba import cuda\n",
    "\n",
    "print(np.__version__)\n",
    "print(numba.__version__)\n",
    "\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\anaconda3\\envs\\numba_env\\lib\\site-packages\\numba\\cuda\\dispatcher.py:539: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 + 7.0 = 9.0\n"
     ]
    }
   ],
   "source": [
    "# Example 1.1: Add scalars\n",
    "@cuda.jit\n",
    "def add_scalars(a, b, c):\n",
    "    c[0] = a + b\n",
    "\n",
    "dev_c = cuda.device_array((1,), np.float32)\n",
    "\n",
    "add_scalars[1, 1](2.0, 7.0, dev_c)\n",
    "\n",
    "c = dev_c.copy_to_host()\n",
    "print(f\"2.0 + 7.0 = {c[0]}\")\n",
    "#  2.0 + 7.0 = 9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  2.  4.  6.  8. 10. 12. 14. 16. 18. 20. 22. 24. 26. 28. 30. 32. 34.\n",
      " 36. 38.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\anaconda3\\envs\\numba_env\\lib\\site-packages\\numba\\cuda\\dispatcher.py:539: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\rodri\\anaconda3\\envs\\numba_env\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# Example 1.2: Add arrays\n",
    "@cuda.jit\n",
    "def add_array(a, b, c):\n",
    "    i = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    if i < a.size:\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "N = 20\n",
    "a = np.arange(N, dtype=np.float32)\n",
    "b = np.arange(N, dtype=np.float32)\n",
    "dev_c = cuda.device_array_like(a)\n",
    "\n",
    "add_array[4, 8](a, b, dev_c)\n",
    "\n",
    "c = dev_c.copy_to_host()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_a = cuda.to_device(a)\n",
    "dev_b = cuda.to_device(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  2.  4.  6.  8. 10. 12. 14. 16. 18. 20. 22. 24. 26. 28. 30. 32. 34.\n",
      " 36. 38.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri\\anaconda3\\envs\\numba_env\\lib\\site-packages\\numba\\cuda\\dispatcher.py:539: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# Example 1.3: Add arrays with cuda.grid\n",
    "@cuda.jit\n",
    "def add_array(a, b, c):\n",
    "    i = cuda.grid(1)\n",
    "    if i < a.size:\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "add_array[4, 8](dev_a, dev_b, dev_c)\n",
    "\n",
    "c = dev_c.copy_to_host()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1_000_000\n",
    "a = np.arange(N, dtype=np.float32)\n",
    "b = np.arange(N, dtype=np.float32)\n",
    "\n",
    "dev_a = cuda.to_device(a)\n",
    "dev_b = cuda.to_device(b)\n",
    "dev_c = cuda.device_array_like(a)\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (N + (threads_per_block - 1)) // threads_per_block\n",
    "# Note that\n",
    "#     blocks_per_grid == ceil(N / threads_per_block)\n",
    "# ensures that blocks_per_grid * threads_per_block >= N\n",
    "\n",
    "add_array[blocks_per_grid, threads_per_block](dev_a, dev_b, dev_c)\n",
    "\n",
    "c = dev_c.copy_to_host()\n",
    "np.allclose(a + b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1.4: Add arrays with grid striding\n",
    "@cuda.jit\n",
    "def add_array_gs(a, b, c):\n",
    "    i_start = cuda.grid(1)\n",
    "    threads_per_grid = cuda.blockDim.x * cuda.gridDim.x\n",
    "    for i in range(i_start, a.size, threads_per_grid):\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid_gs = 32 * 80  # Use 32 * multiple of streaming multiprocessors\n",
    "# 32 * 80 * 256 < 1_000_000 so one thread will process more than one array element\n",
    "\n",
    "add_array_gs[blocks_per_grid_gs, threads_per_block](dev_a, dev_b, dev_c)\n",
    "c = dev_c.copy_to_host()\n",
    "np.allclose(a + b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
